{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Due Thursday, December 12th at 5:00 PM EDT, no extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zifan Jiang. Worked with Muyang Guo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running a community benchmark (15 pts)\n",
    "\n",
    "For those working in pairs, one version of this question will be graded.\n",
    "\n",
    "You are asked to take all of the necessary steps to run a meaningful benchmark code on pace-ice.  First we should talk about what a meaningful benchmark is.\n",
    "\n",
    "### A meaningful benchmark should:\n",
    "\n",
    "### a. Help someone working in a non-HPC domain understand / predict how useful a particular machine is to solving their problem.\n",
    "\n",
    "### b. Report a machine-independent measure of performance, to allow for fair comparison and portability.\n",
    "\n",
    "### c. Have an algorithm-independent statement of what the problem is (i.e. phrased in terms of inputs and outputs), to avoid artificially constraining the implementations.\n",
    "\n",
    "### d. Be as simple as possible, so that the results of the benchmark are explainable and reproducible.\n",
    "\n",
    "With these criteria in mind, you are welcome to select any accepted community benchmark with an open-source implementation.\n",
    "\n",
    "- The benchmark implementation must be *open*, so that we may see what exactly is being run.\n",
    "- An \"accepted community\" benchmark should ideally have a website describing itself, publishd benchmark results, and (ideally) a peer-reviewed in-depth description.\n",
    "\n",
    "\n",
    "### Here are some recommendations that you could choose from:\n",
    "\n",
    "### [HPLinpack](http://www.netlib.org/benchmark/hpl/): Dense Linear Algebra\n",
    "\n",
    "### [HPCG](http://hpcg-benchmark.org/): Iterative Sparse Linear Algebra\n",
    "\n",
    "### [Graph500](https://graph500.org/): Data-Intensive Graph Algorithms\n",
    "\n",
    "### [HPGMG](http://crd.lbl.gov/departments/computer-science/PAR/research/hpgmg/): Multilevel PDE Solvers\n",
    "\n",
    "### [LAMMPS](https://lammps.sandia.gov/index.html) ([benchmarks](https://lammps.sandia.gov/bench.html)): Molecular Dynamics\n",
    "\n",
    "### [TensorFlow](tensorflow.org) ([benchmarks](https://github.com/tensorflow/benchmarks)): Machine Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (1 pts):** In a cell below, tell me which benchmark you are choosing.  Provide a link.  If the benchmark is actually a suite of benchmarks, tell me which one you would like to focus on.  If there are citations for the benchmark, give me those, too, please.  After that, give:\n",
    "\n",
    "- As complete a description as possible of the *problem* being solved.  Include scaling parameters like problem size $N$, and any other \"free\" parameters that can change between different runs of the benchmark.\n",
    "\n",
    "- As complete a description as possible of the *value* of the benchmark: what quantity is being reported?\n",
    "\n",
    "Then, tell me which type of pace-ice node you intend to use to test the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Answer:\n",
    "\n",
    "We chose the HPLinpack Benchmark [HPLinpack](http://www.netlib.org/benchmark/hpl/)<br/>\n",
    "HPL - A Portable Implementation of the High-Performance Linpack, Benchmark for Distributed-Memory Computers <br/>\n",
    "\n",
    "And the versioning is: <br/>\n",
    "HPL 2.3 - by Antoine Petitet, Clint Whaley, Jack Dongar <br/>\n",
    "\n",
    "> HPL is a High-Performance Linpack benchmark implementation.\n",
    "\n",
    "> HPL is written in a portable ANSI C and requires an MPI implementation as well as either BLAS or VSIPL library. Such choice of software dependencies gives HPL both portability and performance.\n",
    "\n",
    "> The HPL package provides a testing and timing program to quantify the accuracy of the obtained solution as well as the time it took to compute it. The best performance achievable by this software on your system depends on a large variety of factors. Nonetheless, with some restrictive assumptions on the interconnection network, the algorithm described here and its attached implementation are scalable in the sense that their parallel efficiency is maintained constant with respect to the per processor memory usage.\n",
    "\n",
    "In our implementations for this project, <br/>\n",
    "\n",
    "All the following packages are included in the module mkl/19.0.\n",
    "\n",
    "- HPL rely on an efficient implementation of the Basic Linear Algebra Subprograms (BLAS) (http://www.netlib.org/blas/blas-3.8.0.tgz)\n",
    "\n",
    "- And MPI module we already had as mvapich2 2.3, a MPI based software package, alternative versionings can be found at (http://mvapich.cse.ohio-state.edu/downloads/)\n",
    "\n",
    "\n",
    "\n",
    "For problems to be solved: <br/>\n",
    "> The code solves a uniformely random system of  linear equations and reports time and floating-point execution rate using a standard formula for operation count. Specifically, HPL generates a linear system of equations of order n and solves it using LU decomposition with partial row pivoting.<br/>\n",
    "For N Problem size, it will need (2/3 * N^3－2N^2) steps to compute, and it measure the computational time performing such (2/3 * N^3－2N^2) steps operations. And then calculated the FLOPs. <br/>\n",
    "Since HPL performs computation on an N x N array of Double Precision (DP) elements, and that each double precision element requires sizeof(double) = 8 bytes, the memory consumed for a problem size of N is 8N^2.<br/>\n",
    "NB=192 for the broadwell processors.<br/>\n",
    "P and Q, knowing that the product P x Q SHOULD typically be equal to the number of MPI processes.<br/>\n",
    "\n",
    "\n",
    "> Here are the parameters:\n",
    "```\n",
    "T/V    : Wall time / encoded variant.\n",
    "N      : The order of the coefficient matrix A.\n",
    "NB     : The partitioning blocking factor.\n",
    "P      : The number of process rows.\n",
    "Q      : The number of process columns.\n",
    "Time   : Time in seconds to solve the linear system.\n",
    "Gflops : Rate of execution for solving the linear system.\n",
    "```\n",
    "\n",
    "For values of the benchmark: <br/>\n",
    "> The HPL benchmark is used as reference benchmark to provide data for the Top500 list and thus rank to supercomputers worldwide.It has the testing program to quantify the accuracy of the obtained solution as well as the time it took to compute it. Generally It reports the computation time per node, and the FLOPS rate. \n",
    "\n",
    "\n",
    "Which nodes on pace-ice we intended to use?<br/>\n",
    "\n",
    "> We chose the nodes on pace-ice clusters with the CPU only.  <br/>\n",
    "They are the ones with 28 CPU cores (14 physical cores), Model is  Intel(R) Xeon(R) CPU E5-2680 v4 @2.40GHz machines. <br/>\n",
    "We intended to perform benchmark tests on one node and four nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (4 pts):** In your own words, give me your assessment of the quality of the benchmark according to the four points (a), (b), (c), and (d) above.\n",
    "\n",
    "- a. Describe some applications where the benchmark problem is relevant.  Benchmarks must walk a fine line between being to specific to one application but very predictive, versus being general to lots of applications while being too simple to predict the performance of any application very well.  Do you think the benchmark you chose does a good job with this balance?\n",
    "\n",
    "- b. What assumptions does your benchmark make about the kind of machine that it is run on?  Do you think that those assumptions are reasonable?  Let's make this question very concrete: let's say you have access to [TaihuLight](https://en.wikipedia.org/wiki/Sunway_TaihuLight), whose nodes are neither really CPUs or GPUs, but somewhere in between.  Could your benchmark run on this machine?  If not, propose a way that you could change the benchmark to make it more portable.\n",
    "\n",
    "- c. How exactly does your benchmark specify the way the problem is solved?  If your benchmark is for a particular algorithm or a particular code, do you think that the results of the benchmark would help you predict the performance of a different code/algorithm solving the same problem on the same machine?\n",
    "\n",
    "- d. One measure of the complexity of a benchmark is how difficult it would be to write a reference implementation from scratch (one that solves the problem, if not in a \"high-performance\" way).  If you had to guess, how big would a team have to be do that: (i) one dedicated programmer; (ii) a team of about a dozen (like a research lab); (iii) an Organization (like a division of a company or a government agency)?  Give your reasoning (by, e.g. measuring lines of code in the implementation you will be working with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Answer:\n",
    "\n",
    "- a: <br/>\n",
    "The HPLinpack (HPL) as a scalable addition to the original Linpack benchmark suites, due to its parallel computation efficiency remaining constant respect to per process memory usage, succeeded in scalability. And it outputs only one single number in time and FLOPS, making this benchmark results are easily compared with different machines.<br/>\n",
    "In this sense, the benchmark is very good at being general to lots of applications. <br/>\n",
    "However, it only tests the **dense linear system**, omitted a fact that there are many more other operations can be taken into account to the performance evaluation. Thus, the best results can be achieved by fine tuning for a particular machine targeting to solving the dense linear system, which caused the results shown are biased. <br/>\n",
    "That is to say, the benchmark is too simple for all kinds of applications testing, but can be very predictive for machines or applications specificially for dense linear system problem solving. <br/>\n",
    "\n",
    "\n",
    "- b: <br/>\n",
    "HPL has been designed as a portable benchmark to perform well for large problem sizes on hundreds of nodes and more.\n",
    "As users, the main algorithm does not need to be modified to run, and it only relies on MPI and BLAS enviroment. So,  theoretically it can be run on clusters with GPU, or CPU or hybrid. But of course, the algorithm needs to be optimized to achieve the best performance results for differerent machine configs. For example, Nvidia, has its own version for HPL benchmarks to run on Nvidia GPUs. And for case of **TianhuLight** , it actually is the third place of the HPLinpark top 500 at the moment, with a results of Peak rate at 125,435.9 TFlop/s running HPL.<br/>\n",
    "\n",
    "\n",
    "\n",
    "- c: <br/>\n",
    "HPL main algorithm code solves a uniformely random system of linear equations and reports time and floating-point execution rate using a standard formula for operation count. Specifically, HPL generates a linear system of equations of order n and solves it using LU decomposition with partial row pivoting. \n",
    "For n Problem size, it will need (2/3 * N^3－2N^2) steps to compute, and it measure the computational time performing such (2/3 * N^3－2N^2) steps operations. And then calculated the FLOPs. <br/>\n",
    "As I reckon, HPL can provide a reference benchmark to predict the machine computation speed, particularly for solving dense linear system type problems. It emphasized the computing part, yet not emphasize enough the interconnection part, thus, If the problems are heavily relied on communications, then the benchmark is not well suited. On the other hand, it the problems are relied on computation mostly, the benchmark results can be useful for predicting. \n",
    "\n",
    "- d: <br/>\n",
    "HPL is the portable reference implementation of HPLinpack benchmarks. To build something similar like HPL from scratch, to estimate the time and labor, I examined the HPL source code and main algorithm (https://www.netlib.org/benchmark/hpl/algorithm.html), the original source code has more than 10000 lines, and the algorithm is straightforward. By my measure, I would assume that building something like a reference implementation from stratch will need a research lab(dozen of people), work in a month or several months to finish.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (1 pts):** Try to prepare for some of the logistics ahead of you.  Answer the following questions:\n",
    "\n",
    "- a. Where / how will you obtain the source for the benchmark driver and implementation that you will be using? (Regarding how: is it a tarball, repository, or other?)\n",
    "\n",
    "- b. What software environments will you need to build and run the benchmark? (e.g. Does it use raw `make`? Autotools?  CMake?  Is it python/pip/conda?  Does it need MPI?  OpenMP?  Cuda?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Answer:\n",
    "- a: <br/>\n",
    "The source code can be obtained from (http://www.netlib.org/benchmark/hpl/hpl-2.3.tar.gz) as a tar, and the dependency packages are BLAS (http://www.netlib.org/blas/blas-3.8.0.tgz) and mpi (https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.2.tar.gz). However, in practice, we directly loaded the module installed on PACE. The detailed process can be found in 1.4 answers, I have documented a step by step workflow for how to successfully install and run the benchmark.\n",
    "\n",
    "\n",
    "- b: <br/>\n",
    "The benchmark needs to be run on a computing node,\n",
    "the following modules need to be loaded:\n",
    "```\n",
    "module use $CSE6230_DIR/modulefiles\n",
    "module load cse6230/core\n",
    "module load cse6230/mkl/19.0\n",
    "```\n",
    "The benchmark can be made locally once downloaded, and it required MPI packages to successfully run. In our case, we already have the MPI software installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (1 pts):** Successfully install and run your benchmark\n",
    "\n",
    "Include in this directory an example **job submission script** that runs your benchmark code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 Benchmark installation:\n",
    "Create a directory named \"hpl\" under \"$CSE6230_DIR\"/final, to contain all the neccessary files for the benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About Downloading the packages\n",
    "\n",
    "hpl-2.3.tar.gz can be downloaded at (http://www.netlib.org/benchmark/hpl/hpl-2.3.tar.gz) <br/>\n",
    "the benchmark tool is hpl-2.3, but blas will be one of the neccessary package to successfully make the executionables \n",
    "BLAS. In our project, the linear algebra package is contained with the intel mkl module, we do not neccessarily install it from scratch.\n",
    "\n",
    "##### About building the tool packages\n",
    "###### Load Modules\n",
    "\n",
    "It is always needed to load the neccessary modules first, here to ease the touble, we always load :\n",
    "```bash\n",
    "module load cse6230/core\n",
    "module load mkl/19.0\n",
    "```\n",
    "\n",
    "###### HPL:\n",
    "\n",
    "After downloading and uploads the packages to the final/hpl folder:\n",
    "\n",
    "`mguo34@coc-ice:~/cse6230/final/hpl$ tar -xzvf hpl-2.3.tar.gz` <br/>\n",
    "\n",
    "`mguo34@coc-ice:~/cse6230/final/hpl$ cd hpl-2.3/setup` <br/>\n",
    "\n",
    "To generate a template:<br/>\n",
    "\n",
    "`mguo34@coc-ice:~/cse6230/final/hpl/hpl-2.3/setup$ sh make_generic ` <br/>\n",
    "\n",
    "Then have our make file named as Make.Linux, and move it to the higher level directory <br/>\n",
    "\n",
    "`mguo34@coc-ice:~/cse6230/final/hpl/hpl-2.3/setup$  cp Make.UNKNOWN ../Make.Linux  ` <br/>\n",
    "\n",
    "And go back to hpl/hpl-2.3/ directory <br/>\n",
    "`mguo34@coc-ice:~/cse6230/final/hpl/hpl-2.3/setup$ cd .. ` <br/>\n",
    "\n",
    "Now we have a Make.Linux template to work on. \n",
    "\n",
    "```bash\n",
    "mguo34@coc-ice:~/cse6230/final/hpl/hpl-2.3$ ls\n",
    "acinclude.m4  BUGS          config.sub    COPYRIGHT  INSTALL     Makefile.am  Make.top  README   THANKS\n",
    "aclocal.m4    ChangeLog     configure     depcomp    install-sh  Makefile.in  man       setup    TODO\n",
    "AUTHORS       compile       configure.ac  HISTORY    lib         Make.Linux   missing   src      TUNING\n",
    "bin           config.guess  COPYING       include    Makefile    makes        NEWS      testing  www\n",
    "\n",
    "```\n",
    "Open with any editors, it should be showing a template, yet not completed, we need add dependecy patch to make sure the tool can be successfully built. it will need mvapich2, BLAS, and the hpl <br/>\n",
    "\n",
    "```bash\n",
    "mguo34@coc-ice:~/cse6230/final/hpl/hpl-2.3$ vi Make.Linux\n",
    "```\n",
    "\n",
    "\n",
    "###### Linear Algebra Package:\n",
    "The linear algebra package is already contained within the mkl/19.0 module, we will simply load this module, and modify the dependecy in Make.Linux file, like the description below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MODIFY MAKE FILE:\n",
    "After successfully get all the neccessary files, we modified package paths and complier/linkers choice in Make.Linux config file:\n",
    "```bash\n",
    "TOPdir       = $(HOME)/cse6230-hw/final/hpl/hpl-2.3\n",
    "MPdir        = /usr/local/pacerepov1/intel/compiler/16.0/impi/5.1.1.109\n",
    "MPinc        = -I $(MPdir)/include64\n",
    "MPlib        = $(MPdir)/intel64/lib/libmpi_mt.so\n",
    "Adir        = $(MKLROOT)\n",
    "ifndef  LAinc\n",
    "LAinc        = -I $(LAdir)/include/\n",
    "endif\n",
    "ifndef  LAlib\n",
    "LAlib        = -L$(LAdir)/lib/intel64 \\\n",
    "               -Wl,--start-group \\\n",
    "               $(LAdir)/lib/intel64/libmkl_intel_lp64.a \\\n",
    "               $(LAdir)/lib/intel64/libmkl_intel_thread.a \\\n",
    "               $(LAdir)/lib/intel64/libmkl_core.a \\\n",
    "               -Wl,--end-group -lpthread -ldl\n",
    "endif\n",
    "\n",
    "CC       = mpiicc\n",
    "CCNOOPT  = $(HPL_DEFS)\n",
    "OMP_DEFS = -qopenmp\n",
    "CCFLAGS  = $(HPL_DEFS) -O3 -w -ansi-alias -i-static -z noexecstack -z relro -z now -nocompchk\n",
    "LINKER       = $(CC)\n",
    "LINKFLAGS    = $(CCFLAGS) $(OMP_DEFS) -mt_mpi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compile into executable:\n",
    "Now go to the hpl-2.3 directory and start compiling: <br/>\n",
    "First Make clean:<br/>\n",
    " `mguo34@coc-ice:~/cse6230/final/hpl/hpl-2.3$ make arch=Linux clean_arch_all`<br/>\n",
    "Then Make:<br/>\n",
    " `mguo34@coc-ice:~/cse6230/final/hpl/hpl-2.3$ make arch=Linux`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 load modules and run a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module load cse6230/core\n",
    "module load mkl/19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU=$(cat /proc/cpuinfo | grep \"model name\" | tail -1)\n",
    "COUNT=$(cat /proc/cpuinfo | grep processor | wc -l)\n",
    "MEM=$(cat /proc/meminfo |grep MemTotal | tail -1)\n",
    "echo \"CPU : $CPU\"\n",
    "echo \"Total CPU Cores : $COUNT\"\n",
    "echo \"$MEM\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CPU : model name\t: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz\n",
    "Total CPU Cores : 28\n",
    "MemTotal:       132176424 kB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to HPL suggestion, a good N should makes the program use 90% of total memory, which is $sqrt(130844220*1024/8)*0.9=116,472$. However, for testrun purpose, it takes too long to compute. So we selected a N=19200 and N=38400, which is about ten and five times smaller than that. For Block size, we also followed the recommendation to choose a value in the [32 , 256] interval. In our example, we selected 64 and 192. Because the example test was performed on one node (chip), each core can be considered closely connected. Hence, 4 by 7 grid was set.   \n",
    "  \n",
    "When submitting to compute node with a script (**`\"$CSE6230_DIR\"/final/script/hpl_singleNode.sh**), we tested from 19200 to 153600 to see how different size (Ns) affect the performance. **Example job submission script can be found in `\"$CSE6230_DIR\"/final/script/hpl_example.sh`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018\n",
      "Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK\n",
      "Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK\n",
      "Modified by Julien Langou, University of Colorado Denver\n",
      "================================================================================\n",
      "\n",
      "An explanation of the input/output parameters follows:\n",
      "T/V    : Wall time / encoded variant.\n",
      "N      : The order of the coefficient matrix A.\n",
      "NB     : The partitioning blocking factor.\n",
      "P      : The number of process rows.\n",
      "Q      : The number of process columns.\n",
      "Time   : Time in seconds to solve the linear system.\n",
      "Gflops : Rate of execution for solving the linear system.\n",
      "\n",
      "The following parameter values will be used:\n",
      "\n",
      "N      :   19200    38400 \n",
      "NB     :      64      192 \n",
      "PMAP   : Row-major process mapping\n",
      "P      :       4 \n",
      "Q      :       7 \n",
      "PFACT  :   Right \n",
      "NBMIN  :       4 \n",
      "NDIV   :       2 \n",
      "RFACT  :   Crout \n",
      "BCAST  :  1ringM \n",
      "DEPTH  :       0 \n",
      "SWAP   : Mix (threshold = 64)\n",
      "L1     : transposed form\n",
      "U      : transposed form\n",
      "EQUIL  : yes\n",
      "ALIGN  : 8 double precision words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- The matrix A is randomly generated for each test.\n",
      "- The following scaled residual check will be computed:\n",
      "      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )\n",
      "- The relative machine precision (eps) is taken to be               2.220446e-16\n",
      "- Computational tests pass if scaled residuals are less than                16.0\n",
      "\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       19200    64     4     7               8.38             5.6287e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:32:05 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:32:14 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.57135916e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       19200   192     4     7               6.31             7.4785e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:32:14 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:32:21 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.73935730e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       38400    64     4     7              60.83             6.2057e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:32:22 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:33:23 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.37495714e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       38400   192     4     7              43.81             8.6164e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:33:25 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:34:09 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.34900801e-03 ...... PASSED\n",
      "================================================================================\n",
      "\n",
      "Finished      4 tests with the following results:\n",
      "              4 tests completed and passed residual checks,\n",
      "              0 tests completed and failed residual checks,\n",
      "              0 tests skipped because of illegal input values.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "End of Tests.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# testrun (for simplicity, only partial test is shown below)\n",
    "cd /nv/coc-ice/zjiang333/cse6230-hw/final/hpl/hpl-2.3/bin/Linux/\n",
    "mpirun -np 28 ./xhpl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 (3 pts):** Develop a performance model for your benchmark\n",
    "\n",
    "In 1.1, you chose a performance metric of your benchmark, let's call it $V$.  Your benchmark will solve a problem with some parameters (problem size, the choice of matrix / network / etc.), let's call those parameters $N$.  The node that you chose to run on will have some machine parameters (The number of cores, the type of GPU, the bandwidth from main memory, etc., etc.), let's call them $P$.  Give an expression \n",
    "for $V(N,P)$ for your benchmark, and describe how you arrived at it.  You should use your discretion when choosing the level of detail.  If it is hard to develop a closed-form performance model for the whole benchmark, but there are a few key kernels that happens repeatedly in your benchmark (a stencil application, an iteration of stochastic gradient descent, etc.), you can give performance models for those benchmarks(s) instead.\n",
    "\n",
    "If it is difficult to formulate your expression in terms of machine parameters, try to develop an expression\n",
    "with coefficients that measure the rates at which the machine can do some lower-level operations (for example, the rate at which a GPU can sum an array).  If you have these coefficients, you should give a plausible description for how the architecture of the machine affects those rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6 (2 pts):** Gather statistics for the performance metric\n",
    "\n",
    "Include in this directory the **job script(s)** that you use to gather statistics for the performance metric on pace-ice.  Additionally, describe what steps you've taken to ensure the quality of the statistics: how are you accounting for variability / noise?  Does your benchmark show different performance on the first run than on subsequent runs?\n",
    "\n",
    "If you are running your benchmark for multiple problem instances ($N$), include a plot of the performance metric for the different problem instances. (You can include error bars for maximum/minimum values of the performance metric for the same problem instance to convey variability.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts used can found in `cse6230-hw/final/script/` and named as `hpl_fullscale*.sh`. To ensure the quality of statistics, we have run the benchmark at the same setting for five times. Below is an example of running HPL benchmark on four nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018\n",
      "Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK\n",
      "Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK\n",
      "Modified by Julien Langou, University of Colorado Denver\n",
      "================================================================================\n",
      "\n",
      "An explanation of the input/output parameters follows:\n",
      "T/V    : Wall time / encoded variant.\n",
      "N      : The order of the coefficient matrix A.\n",
      "NB     : The partitioning blocking factor.\n",
      "P      : The number of process rows.\n",
      "Q      : The number of process columns.\n",
      "Time   : Time in seconds to solve the linear system.\n",
      "Gflops : Rate of execution for solving the linear system.\n",
      "\n",
      "The following parameter values will be used:\n",
      "\n",
      "N      :   19200    38400    57600    76800    96000   115200 \n",
      "NB     :     192 \n",
      "PMAP   : Row-major process mapping\n",
      "P      :       8 \n",
      "Q      :      14 \n",
      "PFACT  :   Right \n",
      "NBMIN  :       4 \n",
      "NDIV   :       2 \n",
      "RFACT  :   Crout \n",
      "BCAST  :  1ringM \n",
      "DEPTH  :       0 \n",
      "SWAP   : Mix (threshold = 64)\n",
      "L1     : transposed form\n",
      "U      : transposed form\n",
      "EQUIL  : yes\n",
      "ALIGN  : 8 double precision words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- The matrix A is randomly generated for each test.\n",
      "- The following scaled residual check will be computed:\n",
      "      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )\n",
      "- The relative machine precision (eps) is taken to be               2.220446e-16\n",
      "- Computational tests pass if scaled residuals are less than                16.0\n",
      "\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       19200   192     8    14               2.67             1.7684e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:12:52 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:12:55 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.64198128e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       38400   192     8    14              14.03             2.6908e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:12:55 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:13:09 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.22453577e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       57600   192     8    14              41.69             3.0563e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:13:10 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:13:52 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.34435654e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       76800   192     8    14              94.65             3.1905e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:13:54 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:15:28 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   9.74225766e-04 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       96000   192     8    14             177.22             3.3282e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:15:31 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:18:28 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   8.39070264e-04 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4      115200   192     8    14             299.85             3.3992e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:18:32 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:23:32 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.10942470e-03 ...... PASSED\n",
      "================================================================================\n",
      "\n",
      "Finished      6 tests with the following results:\n",
      "              6 tests completed and passed residual checks,\n",
      "              0 tests completed and failed residual checks,\n",
      "              0 tests skipped because of illegal input values.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "End of Tests.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cd /nv/coc-ice/zjiang333/cse6230-hw/final/hpl/hpl-2.3/bin/Linux/\n",
    "mpirun -np 112 ./xhpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.7 (3 pts):** Compare your performance model to your statistics\n",
    "\n",
    "If your performance model allowed you to make a concrete prediction of $V(N,P)$ before running the benchmark,\n",
    "compare (in a table or plot) the predictions and the actual measurements.\n",
    "\n",
    "If your performance model includes coefficients that could not be estimated ahead of time, use measurements gathered during your experiments to get empirical values of those coefficients to fit the model to the data.  Once you do this, answer the following question: is there a plausible explanation (in terms of the architecture of the machine and the nature of the algorithm) for why these coefficients have the value that they do?\n",
    "\n",
    "Present additional timings and/or machine performance metrics (either for the full benchmark or key kernels) and make a case for why you think they best demonstrate what the biggest bottleneck to the performance of the benchmark is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paper report (5 pts)\n",
    "\n",
    "**Completed separately, not as a team**\n",
    "\n",
    "Choose one of the two Gordon Bell Prize finalists this year:\n",
    "\n",
    "- [A data-centric approach to extreme-scale ab initio dissipative quantum transport simulations](https://dl.acm.org/citation.cfm?id=3357156)\n",
    "- [Fast, scalable and accurate finite-element based ab initio calculations using mixed precision computing: 46 PFLOPS simulation of a metallic dislocation system](https://dl.acm.org/citation.cfm?id=3357157)\n",
    "\n",
    "Read the paper, and answer the following questions:\n",
    "\n",
    "Answers to the first paper are as following:  \n",
    "\n",
    "**a. What is the problem being solved?**   \n",
    "  \n",
    "A: It improves the computational efficiency of ab initio quantum transport (QT) solver. Large-scale QT simulations are bound by both communication volume and memory requirements. The former inhibits strong scaling, as simulation time includes nanostructure-dependent point-to-point communication patterns, which become infeasible when increasing node count. The memory bottleneck is a direct result of the former.  \n",
    "\n",
    "**b.What are the most important kernels of the algorithm for solving the problem? (In this context consider a kernel to be a subproblem that is relevant to more than just the specific problem from a.)**   \n",
    "   \n",
    "A: They showed that the key to eliminating the scaling bottleneck is in formulating a communication-avoiding algorithm, which is tightly coupled with recovering local and global data dependencies of the application.  \n",
    "\n",
    "The most important kernel is solving scattering self-energies (SSE) equations $\\Sigma (E,k_z)$ and $\\Pi (\\omega, q_z)$. 95% of simulation time is dedicated to SSE, regardless of the number of used cores/nodes, among which ∼60% for the communication between the different MPI tasks.  \n",
    "\n",
    "The method they used in the paper was a Data-Centric(DaCe) SSE, which reduces magnitudes of data movement and communications between ranks.  \n",
    "**c. What about the important kernels and/or the size of the problem make this a challenging problem?**   \n",
    "  \n",
    "A: SSE are required to be evaluated for all $\\Sigma (E,k_z)$ and $\\Pi (\\omega, q_z)$, so the computation cost increases with the number of atoms simulated. However, understanding realistic FinFET transistors requires simulations with $N_a ≥ 10, 000$ atoms and high accuracy. The SSE phase comsumption in SOTA result (OMEN) before this DaCe approch grows with the cores used, which could be attributed to communication overhead. To simulate the disired number of atoms with required accuracy, two orders of magnitude improvement in computation time per atom has to be made.    \n",
    "**d. Summarize the innovation of this paper.**   \n",
    "  \n",
    "A: The innovations reported includes: \n",
    "1. Material was staged and chunked broadcasted to nodes, so data ingestion was ~15 times faster.  \n",
    "2. Domain decomposation of SSE computation was tiled by atom position on the nano device instead of being controled by the top map as in OMEN. This modification reduces the movement needed and the complexity in MPI communications. \n",
    "3. Many kinds of computation were pipelined on GPU. For instance, contour integral calculation for boundary condition was pipelined. Copy/compute and compute/compute overlap were automatically generated by DaCe framework. \n",
    "4. In Green's Function (GF) computation, they investigated different DaCe transformation on sparse Hamiltonian blocks using a combination of sparse and dense matrices. \n",
    "5. Dataflow in SSE kernel was reformulated to provide speedup. For example, using map fission and data layout transformation, they reshaped the job into a stencil-like strided-batched dense multiplication, which yeiled ~5x speedup over cuBLAS.   \n",
    "   \n",
    "**e. What model is used that combines the problem parameters and machine parameters to predict performance?**  \n",
    "  \n",
    "A: For local computation, per-component benchmarks like matrix multiplication were performed on single node or GPU to provide actual local computation cost on certain system, which then help predict the overall perfrormance. For communication, by dividing the amount of data each node must sned by the injection bandwidth of that node, they preidcted a lower bounds for the completion time of each call. Overall performance wasn't predicted in the paper. Instead, actual performance is reported, and efficiency was calculated by dividing the achieved flop/s by peak flop/s measured. \n",
    "\n",
    "**f. Any paper that is submitted for a prize contains some marketing, and maybe some attempts to [fool the masses](https://blogs.fau.de/hager/archives/category/fooling-the-masses).  If you could ask the authors to submit one additional figure with the performance measurements of an experiment, what would you choose, and why?**  \n",
    "  \n",
    "A: I would ask the authors to add a figure about how the performance of cuPLAS and DaCe (SBSMM) change with the size of individual matrices used in the strided matrix multiplication. Because they claimed that cuBLAS excessively padded small matrices, hence the useful flops is low, although the seemingly high peak percentage. If the figure does support their hypothesis, we should expect something like the performance of cuBLAS increases with the size of matrices included while the DaCe doesn't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
