{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Due Thursday, December 12th at 5:00 PM EDT, no extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worked with Muyang Guo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running a community benchmark (15 pts)\n",
    "\n",
    "For those working in pairs, one version of this question will be graded.\n",
    "\n",
    "You are asked to take all of the necessary steps to run a meaningful benchmark code on pace-ice.  First we should talk about what a meaningful benchmark is.\n",
    "\n",
    "### A meaningful benchmark should:\n",
    "\n",
    "### a. Help someone working in a non-HPC domain understand / predict how useful a particular machine is to solving their problem.\n",
    "\n",
    "### b. Report a machine-independent measure of performance, to allow for fair comparison and portability.\n",
    "\n",
    "### c. Have an algorithm-independent statement of what the problem is (i.e. phrased in terms of inputs and outputs), to avoid artificially constraining the implementations.\n",
    "\n",
    "### d. Be as simple as possible, so that the results of the benchmark are explainable and reproducible.\n",
    "\n",
    "With these criteria in mind, you are welcome to select any accepted community benchmark with an open-source implementation.\n",
    "\n",
    "- The benchmark implementation must be *open*, so that we may see what exactly is being run.\n",
    "- An \"accepted community\" benchmark should ideally have a website describing itself, publishd benchmark results, and (ideally) a peer-reviewed in-depth description.\n",
    "\n",
    "\n",
    "### Here are some recommendations that you could choose from:\n",
    "\n",
    "### [HPLinpack](http://www.netlib.org/benchmark/hpl/): Dense Linear Algebra\n",
    "\n",
    "### [HPCG](http://hpcg-benchmark.org/): Iterative Sparse Linear Algebra\n",
    "\n",
    "### [Graph500](https://graph500.org/): Data-Intensive Graph Algorithms\n",
    "\n",
    "### [HPGMG](http://crd.lbl.gov/departments/computer-science/PAR/research/hpgmg/): Multilevel PDE Solvers\n",
    "\n",
    "### [LAMMPS](https://lammps.sandia.gov/index.html) ([benchmarks](https://lammps.sandia.gov/bench.html)): Molecular Dynamics\n",
    "\n",
    "### [TensorFlow](tensorflow.org) ([benchmarks](https://github.com/tensorflow/benchmarks)): Machine Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (1 pts):** In a cell below, tell me which benchmark you are choosing.  Provide a link.  If the benchmark is actually a suite of benchmarks, tell me which one you would like to focus on.  If there are citations for the benchmark, give me those, too, please.  After that, give:\n",
    "\n",
    "- As complete a description as possible of the *problem* being solved.  Include scaling parameters like problem size $N$, and any other \"free\" parameters that can change between different runs of the benchmark.\n",
    "\n",
    "- As complete a description as possible of the *value* of the benchmark: what quantity is being reported?\n",
    "\n",
    "Then, tell me which type of pace-ice node you intend to use to test the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (4 pts):** In your own words, give me your assessment of the quality of the benchmark according to the four points (a), (b), (c), and (d) above.\n",
    "\n",
    "- a. Describe some applications where the benchmark problem is relevant.  Benchmarks must walk a fine line between being to specific to one application but very predictive, versus being general to lots of applications while being too simple to predict the performance of any application very well.  Do you think the benchmark you chose does a good job with this balance?\n",
    "\n",
    "- b. What assumptions does your benchmark make about the kind of machine that it is run on?  Do you think that those assumptions are reasonable?  Let's make this question very concrete: let's say you have access to [TaihuLight](https://en.wikipedia.org/wiki/Sunway_TaihuLight), whose nodes are neither really CPUs or GPUs, but somewhere in between.  Could your benchmark run on this machine?  If not, propose a way that you could change the benchmark to make it more portable.\n",
    "\n",
    "- c. How exactly does your benchmark specify the way the problem is solved?  If your benchmark is for a particular algorithm or a particular code, do you think that the results of the benchmark would help you predict the performance of a different code/algorithm solving the same problem on the same machine?\n",
    "\n",
    "- d. One measure of the complexity of a benchmark is how difficult it would be to write a reference implementation from scratch (one that solves the problem, if not in a \"high-performance\" way).  If you had to guess, how big would a team have to be do that: (i) one dedicated programmer; (ii) a team of about a dozen (like a research lab); (iii) an Organization (like a division of a company or a government agency)?  Give your reasoning (by, e.g. measuring lines of code in the implementation you will be working with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (1 pts):** Try to prepare for some of the logistics ahead of you.  Answer the following questions:\n",
    "\n",
    "- a. Where / how will you obtain the source for the benchmark driver and implementation that you will be using? (Regarding how: is it a tarball, repository, or other?)\n",
    "\n",
    "- b. What software environments will you need to build and run the benchmark? (e.g. Does it use raw `make`? Autotools?  CMake?  Is it python/pip/conda?  Does it need MPI?  OpenMP?  Cuda?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (1 pts):** Successfully install and run your benchmark\n",
    "\n",
    "Include in this directory an example **job submission script** that runs your benchmark code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\n",
      "|                                                                         |\n",
      "|       A note about python/3.6:                                          |\n",
      "|       PACE is lacking the staff to install all of the python 3          |\n",
      "|       modules, but we do maintain an anaconda distribution for          |\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\n",
      "|       the overhead with package management, we would much prefer        |\n",
      "|       to maintain python 3 through anaconda.                            |\n",
      "|                                                                         |\n",
      "|       All pace installed modules are visible via the module avail       |\n",
      "|       command.                                                          |\n",
      "|                                                                         |\n",
      " =========================================================================\n"
     ]
    }
   ],
   "source": [
    "module load cse6230/core\n",
    "module load mkl/19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU(s):                28\n",
      "Thread(s) per core:    1\n",
      "Core(s) per socket:    14\n",
      "Socket(s):             2\n",
      "bash: nvidia-smi: command not found\n",
      "This nodes has 28 cores: its architecture is (Manufacturer, Product Id) Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz\n",
      "This node has no GPUs\n",
      "             total       used       free     shared    buffers     cached\n",
      "Mem:          504G        40G       463G        38M       468M        33G\n",
      "-/+ buffers/cache:       6.4G       498G \n",
      "Swap:         2.0G         0B       2.0G \n"
     ]
    }
   ],
   "source": [
    "lscpu | grep -E '^Thread|^Core|^Socket|^CPU\\('\n",
    "nvidia-smi\n",
    "CPU_NAME=`cat /proc/cpuinfo| grep 'model name'|uniq | grep -P -m 1 -o -e \"(?<=model name\\s: ).*\"`\n",
    "CORE_COUNT=`cat /proc/cpuinfo |grep 'model name'|wc -l`\n",
    "GPU_NAME=`nvidia-smi -q|grep 'Product Name'|uniq|grep -o \"Tesla.*\"`\n",
    "GPU_COUNT=`nvidia-smi -q|grep 'Product Name'|wc -l`\n",
    "echo \"This nodes has ${CORE_COUNT} cores: its architecture is (Manufacturer, Product Id) ${CPU_NAME}\"\n",
    "if [[ ! $GPU_COUNT || $GPU_COUNT == 0 ]] ;  then\n",
    "    echo \"This node has no GPUs\"\n",
    "else\n",
    "    echo \"This node has ${GPU_COUNT} GPUs: its/their architecture is (Manufacturer, Product Id) ${GPU_NAME}\"\n",
    "fi\n",
    "\n",
    "free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to HPL suggestion, a good N should makes the program use 90% of total memory, which is $sqrt(130844220*1024/8)*0.9=116,472$. However, it takes too long (>6h) to compute. So we selected a N=11647, which is ten times smaller than that. For Block size, we also followed the recommendation to choose a value in the [32 , 256] interval. In our example, we selected 32, 64, 128, 256. Because the example test was performed on one node (chip), each core can be considered closely connected. Hence, 2 by 4 grid was set. **Example job submission script can be found in `cse6230-hw/final/script/pl_example.sh`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018\n",
      "Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK\n",
      "Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK\n",
      "Modified by Julien Langou, University of Colorado Denver\n",
      "================================================================================\n",
      "\n",
      "An explanation of the input/output parameters follows:\n",
      "T/V    : Wall time / encoded variant.\n",
      "N      : The order of the coefficient matrix A.\n",
      "NB     : The partitioning blocking factor.\n",
      "P      : The number of process rows.\n",
      "Q      : The number of process columns.\n",
      "Time   : Time in seconds to solve the linear system.\n",
      "Gflops : Rate of execution for solving the linear system.\n",
      "\n",
      "The following parameter values will be used:\n",
      "\n",
      "N      :   19200    38400 \n",
      "NB     :      64      192 \n",
      "PMAP   : Row-major process mapping\n",
      "P      :       4 \n",
      "Q      :       7 \n",
      "PFACT  :   Right \n",
      "NBMIN  :       4 \n",
      "NDIV   :       2 \n",
      "RFACT  :   Crout \n",
      "BCAST  :  1ringM \n",
      "DEPTH  :       0 \n",
      "SWAP   : Mix (threshold = 64)\n",
      "L1     : transposed form\n",
      "U      : transposed form\n",
      "EQUIL  : yes\n",
      "ALIGN  : 8 double precision words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- The matrix A is randomly generated for each test.\n",
      "- The following scaled residual check will be computed:\n",
      "      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )\n",
      "- The relative machine precision (eps) is taken to be               2.220446e-16\n",
      "- Computational tests pass if scaled residuals are less than                16.0\n",
      "\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       19200    64     4     7               8.38             5.6287e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:32:05 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:32:14 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.57135916e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       19200   192     4     7               6.31             7.4785e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:32:14 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:32:21 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.73935730e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       38400    64     4     7              60.83             6.2057e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:32:22 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:33:23 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.37495714e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       38400   192     4     7              43.81             8.6164e+02\n",
      "HPL_pdgesv() start time Wed Dec 11 15:33:25 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 15:34:09 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.34900801e-03 ...... PASSED\n",
      "================================================================================\n",
      "\n",
      "Finished      4 tests with the following results:\n",
      "              4 tests completed and passed residual checks,\n",
      "              0 tests completed and failed residual checks,\n",
      "              0 tests skipped because of illegal input values.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "End of Tests.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# testrun \n",
    "cd /nv/coc-ice/zjiang333/cse6230-hw/final/hpl/hpl-2.3/bin/Linux/\n",
    "mpirun -np 28 ./xhpl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 (3 pts):** Develop a performance model for your benchmark\n",
    "\n",
    "In 1.1, you chose a performance metric of your benchmark, let's call it $V$.  Your benchmark will solve a problem with some parameters (problem size, the choice of matrix / network / etc.), let's call those parameters $N$.  The node that you chose to run on will have some machine parameters (The number of cores, the type of GPU, the bandwidth from main memory, etc., etc.), let's call them $P$.  Give an expression \n",
    "for $V(N,P)$ for your benchmark, and describe how you arrived at it.  You should use your discretion when choosing the level of detail.  If it is hard to develop a closed-form performance model for the whole benchmark, but there are a few key kernels that happens repeatedly in your benchmark (a stencil application, an iteration of stochastic gradient descent, etc.), you can give performance models for those benchmarks(s) instead.\n",
    "\n",
    "If it is difficult to formulate your expression in terms of machine parameters, try to develop an expression\n",
    "with coefficients that measure the rates at which the machine can do some lower-level operations (for example, the rate at which a GPU can sum an array).  If you have these coefficients, you should give a plausible description for how the architecture of the machine affects those rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6 (2 pts):** Gather statistics for the performance metric\n",
    "\n",
    "Include in this directory the **job script(s)** that you use to gather statistics for the performance metric on pace-ice.  Additionally, describe what steps you've taken to ensure the quality of the statistics: how are you accounting for variability / noise?  Does your benchmark show different performance on the first run than on subsequent runs?\n",
    "\n",
    "If you are running your benchmark for multiple problem instances ($N$), include a plot of the performance metric for the different problem instances. (You can include error bars for maximum/minimum values of the performance metric for the same problem instance to convey variability.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script used can found in `cse6230-hw/final/script/hpl_fullscale.sh`. To ensure the quality of statistics, we have run the benchmark at the same setting for five times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018\n",
      "Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK\n",
      "Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK\n",
      "Modified by Julien Langou, University of Colorado Denver\n",
      "================================================================================\n",
      "\n",
      "An explanation of the input/output parameters follows:\n",
      "T/V    : Wall time / encoded variant.\n",
      "N      : The order of the coefficient matrix A.\n",
      "NB     : The partitioning blocking factor.\n",
      "P      : The number of process rows.\n",
      "Q      : The number of process columns.\n",
      "Time   : Time in seconds to solve the linear system.\n",
      "Gflops : Rate of execution for solving the linear system.\n",
      "\n",
      "The following parameter values will be used:\n",
      "\n",
      "N      :   19200    38400    57600    76800    96000   115200 \n",
      "NB     :     192 \n",
      "PMAP   : Row-major process mapping\n",
      "P      :       8 \n",
      "Q      :      14 \n",
      "PFACT  :   Right \n",
      "NBMIN  :       4 \n",
      "NDIV   :       2 \n",
      "RFACT  :   Crout \n",
      "BCAST  :  1ringM \n",
      "DEPTH  :       0 \n",
      "SWAP   : Mix (threshold = 64)\n",
      "L1     : transposed form\n",
      "U      : transposed form\n",
      "EQUIL  : yes\n",
      "ALIGN  : 8 double precision words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "- The matrix A is randomly generated for each test.\n",
      "- The following scaled residual check will be computed:\n",
      "      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )\n",
      "- The relative machine precision (eps) is taken to be               2.220446e-16\n",
      "- Computational tests pass if scaled residuals are less than                16.0\n",
      "\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       19200   192     8    14               2.67             1.7684e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:12:52 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:12:55 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.64198128e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       38400   192     8    14              14.03             2.6908e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:12:55 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:13:09 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.22453577e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       57600   192     8    14              41.69             3.0563e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:13:10 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:13:52 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.34435654e-03 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       76800   192     8    14              94.65             3.1905e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:13:54 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:15:28 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   9.74225766e-04 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4       96000   192     8    14             177.22             3.3282e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:15:31 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:18:28 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   8.39070264e-04 ...... PASSED\n",
      "================================================================================\n",
      "T/V                N    NB     P     Q               Time                 Gflops\n",
      "--------------------------------------------------------------------------------\n",
      "WR01C2R4      115200   192     8    14             299.85             3.3992e+03\n",
      "HPL_pdgesv() start time Wed Dec 11 16:18:32 2019\n",
      "\n",
      "HPL_pdgesv() end time   Wed Dec 11 16:23:32 2019\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   1.10942470e-03 ...... PASSED\n",
      "================================================================================\n",
      "\n",
      "Finished      6 tests with the following results:\n",
      "              6 tests completed and passed residual checks,\n",
      "              0 tests completed and failed residual checks,\n",
      "              0 tests skipped because of illegal input values.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "End of Tests.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cd /nv/coc-ice/zjiang333/cse6230-hw/final/hpl/hpl-2.3/bin/Linux/\n",
    "mpirun -np 112 ./xhpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.7 (3 pts):** Compare your performance model to your statistics\n",
    "\n",
    "If your performance model allowed you to make a concrete prediction of $V(N,P)$ before running the benchmark,\n",
    "compare (in a table or plot) the predictions and the actual measurements.\n",
    "\n",
    "If your performance model includes coefficients that could not be estimated ahead of time, use measurements gathered during your experiments to get empirical values of those coefficients to fit the model to the data.  Once you do this, answer the following question: is there a plausible explanation (in terms of the architecture of the machine and the nature of the algorithm) for why these coefficients have the value that they do?\n",
    "\n",
    "Present additional timings and/or machine performance metrics (either for the full benchmark or key kernels) and make a case for why you think they best demonstrate what the biggest bottleneck to the performance of the benchmark is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paper report (5 pts)\n",
    "\n",
    "**Completed separately, not as a team**\n",
    "\n",
    "Choose one of the two Gordon Bell Prize finalists this year:\n",
    "\n",
    "- [A data-centric approach to extreme-scale ab initio dissipative quantum transport simulations](https://dl.acm.org/citation.cfm?id=3357156)\n",
    "- [Fast, scalable and accurate finite-element based ab initio calculations using mixed precision computing: 46 PFLOPS simulation of a metallic dislocation system](https://dl.acm.org/citation.cfm?id=3357157)\n",
    "\n",
    "Read the paper, and answer the following questions:\n",
    "\n",
    "Answers to the first paper are as following:  \n",
    "\n",
    "**a. What is the problem being solved?**   \n",
    "  \n",
    "A: It improves the computational efficiency of ab initio quantum transport (QT) solver. Large-scale QT simulations are bound by both communication volume and memory requirements. The former inhibits strong scaling, as simulation time includes nanostructure-dependent point-to-point communication patterns, which become infeasible when increasing node count. The memory bottleneck is a direct result of the former.  \n",
    "\n",
    "**b.What are the most important kernels of the algorithm for solving the problem? (In this context consider a kernel to be a subproblem that is relevant to more than just the specific problem from a.)**   \n",
    "   \n",
    "A: They showed that the key to eliminating the scaling bottleneck is in formulating a communication-avoiding algorithm, which is tightly coupled with recovering local and global data dependencies of the application.  \n",
    "\n",
    "The most important kernel is solving scattering self-energies (SSE) equations $\\Sigma (E,k_z)$ and $\\Pi (\\omega, q_z)$. 95% of simulation time is dedicated to SSE, regardless of the number of used cores/nodes, among which ∼60% for the communication between the different MPI tasks.  \n",
    "\n",
    "The method they used in the paper was a Data-Centric(DaCe) SSE, which reduces magnitudes of data movement and communications between ranks.  \n",
    "**c. What about the important kernels and/or the size of the problem make this a challenging problem?**   \n",
    "  \n",
    "A: SSE are required to be evaluated for all $\\Sigma (E,k_z)$ and $\\Pi (\\omega, q_z)$, so the computation cost increases with the number of atoms simulated. However, understanding realistic FinFET transistors requires simulations with $N_a ≥ 10, 000$ atoms and high accuracy. The SSE phase comsumption in SOTA result (OMEN) before this DaCe approch grows with the cores used, which could be attributed to communication overhead. To simulate the disired number of atoms with required accuracy, two orders of magnitude improvement in computation time per atom has to be made.    \n",
    "**d. Summarize the innovation of this paper.**   \n",
    "  \n",
    "A: The innovations reported includes: \n",
    "1. Material was staged and chunked broadcasted to nodes, so data ingestion was ~15 times faster.  \n",
    "2. Domain decomposation of SSE computation was tiled by atom position on the nano device instead of being controled by the top map as in OMEN. This modification reduces the movement needed and the complexity in MPI communications. \n",
    "3. Many kinds of computation were pipelined on GPU. For instance, contour integral calculation for boundary condition was pipelined. Copy/compute and compute/compute overlap were automatically generated by DaCe framework. \n",
    "4. In Green's Function (GF) computation, they investigated different DaCe transformation on sparse Hamiltonian blocks using a combination of sparse and dense matrices. \n",
    "5. Dataflow in SSE kernel was reformulated to provide speedup. For example, using map fission and data layout transformation, they reshaped the job into a stencil-like strided-batched dense multiplication, which yeiled ~5x speedup over cuBLAS.   \n",
    "   \n",
    "**e. What model is used that combines the problem parameters and machine parameters to predict performance?**  \n",
    "  \n",
    "A: For local computation, per-component benchmarks like matrix multiplication were performed on single node or GPU to provide actual local computation cost on certain system, which then help predict the overall perfrormance. For communication, by dividing the amount of data each node must sned by the injection bandwidth of that node, they preidcted a lower bounds for the completion time of each call. Overall performance wasn't predicted in the paper. Instead, actual performance is reported, and efficiency was calculated by dividing the achieved flop/s by peak flop/s measured. \n",
    "\n",
    "**f. Any paper that is submitted for a prize contains some marketing, and maybe some attempts to [fool the masses](https://blogs.fau.de/hager/archives/category/fooling-the-masses).  If you could ask the authors to submit one additional figure with the performance measurements of an experiment, what would you choose, and why?**  \n",
    "  \n",
    "A: I would ask the authors to add a figure about how the performance of cuPLAS and DaCe (SBSMM) change with the size of individual matrices used in the strided matrix multiplication. Because they claimed that cuBLAS excessively padded small matrices, hence the useful flops is low, although the seemingly high peak percentage. If the figure does support their hypothesis, we should expect something like the performance of cuBLAS increases with the size of matrices included while the DaCe doesn't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
