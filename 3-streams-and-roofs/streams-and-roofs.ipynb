{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streams and Roofs\n",
    "\n",
    "In this week's assignment we are going to make some roofline diagrams for some $n$-body problems.\n",
    "\n",
    "This week's assignment is meant to be run on a node with a Tesla P100 GPU.\n",
    "\n",
    "A reminder: when you are running a job to complete this week's assignment, you should make sure that you have requested exclusive access to a node, and that you have requested access to all CPU cores of this node.\n",
    "\n",
    "**Due: Thursday, September 12, before class**\n",
    "\n",
    "Let's load in our class module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module use $CSE6230_DIR/modulefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =========================================================================\r",
      "\r\n",
      "|                                                                         |\r",
      "\r\n",
      "|       A note about python/3.6:                                          |\r",
      "\r\n",
      "|       PACE is lacking the staff to install all of the python 3          |\r",
      "\r\n",
      "|       modules, but we do maintain an anaconda distribution for          |\r",
      "\r\n",
      "|       both python 2 and python 3. As conda significantly reduces        |\r",
      "\r\n",
      "|       the overhead with package management, we would much prefer        |\r",
      "\r\n",
      "|       to maintain python 3 through anaconda.                            |\r",
      "\r\n",
      "|                                                                         |\r",
      "\r\n",
      "|       All pace installed modules are visible via the module avail       |\r",
      "\r\n",
      "|       command.                                                          |\r",
      "\r\n",
      "|                                                                         |\r",
      "\r\n",
      " =========================================================================\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!module load cse6230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\r\n",
      "  1) curl/7.42.1\r\n",
      "  2) git/2.13.4\r\n",
      "  3) python/3.6\r\n",
      "  4) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/jupyter/1.0\r\n",
      "  5) intel/16.0\r\n",
      "  6) cuda/8.0.44\r\n",
      "  7) /nv/coc-ice/tisaac3/opt/pace-ice/modulefiles/hpctoolkit/2018.18\r\n",
      "  8) impi/5.1.1.109\r\n",
      "  9) cse6230/core(default)\r\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify that we're running where we expect to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 11 22:04:56 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:81:00.0 Off |                    0 |\r\n",
      "| N/A   27C    P0    25W / 250W |      0MiB / 16280MiB |      0%   E. Process |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now, about the $n$-body simulations we're going to run: a classical $n$-body simulation has each body, or *particle*, interacting with each other, for $n(n+1)/2$ total interactions.  That hardly matches up to the streaming kernels we've been talking about!  So we're going to simplify a bit.\n",
    "\n",
    "We are going to simulate $n$ infinitesimal particles circling around an infinitely massive sun at the origin.  In this system, the sun is unmoved, and the particles are not affected by each other.\n",
    "\n",
    "We're going to normalize our coefficients and say that each particle is an ordinary differential equation with *six* components: three of position $X=(x, y, z)$ and three of velocity $U=(u, v, w)$.  The position, is changed by the velocity, of course, but the velocity changes under acceleration that depends on position:\n",
    "\n",
    "$$\\begin{aligned} \\dot{X} &= V \\\\ \\dot{V} &= - \\frac{X}{|X|^3}.\\end{aligned}$$\n",
    "\n",
    "To discretize this differential equation, we are going to use a time stepping method called the Verlet leap-frog method, which is good for calculating long simulations of stable orbits.  Given a time step length `dt`, our pseudocode for one time step for one particle looks like the following:\n",
    "\n",
    "1. `X += 0.5 * dt * V`\n",
    "2. `R2 = X . X` (dot product)\n",
    "3. `R = sqrt (R2)`\n",
    "4. `IR3 = 1. / (R2 * R)`\n",
    "5. `V -= X * dt * IR3`\n",
    "6. `X += 0.5 * dt * V`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Assuming `sqrt` and `div` count for one flop each, and assuming `x, y, z` and `u, v, w` are **double-precision** floating point\n",
    "numbers, **estimate the arithmetic intensity of a *particle time step***.  You should ignore the time it takes to load `dt`.  Your answer should have units of flops / byte.  Give your answer in a new cell below this one, and show how you arrived at that number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "    1. 3 ADD, 4 MUL, 6 loads, 3 writes: AI= 7/(9*8) = 7/72 = 0.097 \n",
    "    2. 3 MUL, 3 loads, 1 writes: AI = 3/4/8 = 3/32 = 0.094\n",
    "    3. 1 sqrt, 1 load, 1 write: AI = 1 / 2 / 8 = 1/16 = 0.0625\n",
    "    4. 1 MUL, 1 DIV, 2 loads, 1 write: AI = 2 / 3/8 = 1/12 = 0.083\n",
    "    5. 6 MUL, 3 ADD, 7 loads, 3 writes: AI = 9 / 10/8 = 9/80 = 0.1125\n",
    "    6. ... (return to 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Using the peak theoretical **double-precision** flop/s of this node (flop/s on the CPUs and GPU combined), calculated the same way as in the last assignment, and reported peak memory bandwidths from the manufacturers, **estimate the system balance of CPUs and the GPU of this node separately**.  Note that the bandwidth estimate from intel will be for one socket (4 cores) with attached memory, and our node has two such sockets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Peak theoretical double-precision flop/s of this node is 1638(CPU) and 4763 GFLOPS. \n",
    "This server with DDR4 2133 RAM has 68.3 GB/s * 2 sockets = 136.6GB/s bandwidth. Peak GPU meory bandwidth is 720 GB/s.  \n",
    "Therefore, the system balance for CPU is:  \n",
    "$1638 GFLOPS / 136.6GB/s = 11.99 flop / byte $  \n",
    "the system balance for GPU is:  \n",
    "$4763 GFLOPS / 720GB/s = 6.61 flop / byte $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we didn't take the peak flop/s values from the manufacturers at face value, and this week we are not going to take the beak Gbyte/s for granted either.  Last week we used a custom benchmark in our calculations; this week we will use an industry standard: the\n",
    "[STREAM benchmark](https://www.cs.virginia.edu/stream/ref.html).\n",
    "\n",
    "We can run the stream benchmark on the CPUs for this assignment with a makefile target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icc -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream stream.c -DSTREAM_ARRAY_SIZE=40000000\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "./stream\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 40000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 305.2 MiB (= 0.3 GiB).\n",
      "Total memory required = 915.5 MiB (= 0.9 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 1\n",
      "Number of Threads counted = 1\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 33715 microseconds.\n",
      "   (= 33715 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           19631.9     0.032651     0.032600     0.032684\n",
      "Scale:          21091.5     0.030390     0.030344     0.030524\n",
      "Add:            19318.6     0.049977     0.049693     0.051742\n",
      "Triad:          19475.4     0.049353     0.049293     0.049508\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!make runstream STREAM_N=40000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `STREAM_N` argument will control the size of the stream arrays.\n",
    "\n",
    "**Question 3:** Modify the invocation of `make runstreams` by modifying the values of\n",
    "`STREAM_N`, `COPTFLAGS` (optimization flags), `OMP_NUM_THREADS` and/or `OMP_PROC_BIND` (the openMP environment variables) to get the largest streaming bandwidth from main memory that you can for this node.\n",
    "\n",
    "[The OpenMP environment variables were not defined by me in the Makefile: they are environment variables that will be detected by the OpenMP runtime when an OpenMP program begins.  You should put them _before_ the make command, e.g. `OMP_NUM_THREADS=5 make runstream STREAM_N=40000000`]\n",
    "\n",
    "- Follow the directions in the output of the file and make sure you are testing streaming bandwidth from memory and not from a higher level of cache.\n",
    "- You should try to get close to the same bandwidth for all tests:\n",
    "\n",
    "- There are two variables in the openMP environment you should care about, OMP_NUM_THREADS, which is self explanatory, and OMP_PROC_BIND is discussed [here](http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html).  **You should try to use as few threads as possible** to achieve peak bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icc -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream stream.c -DSTREAM_ARRAY_SIZE=4000000\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "./stream\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 4000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 30.5 MiB (= 0.0 GiB).\n",
      "Total memory required = 91.6 MiB (= 0.1 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 32\n",
      "Number of Threads counted = 32\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 779 microseconds.\n",
      "   (= 779 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           49344.8     0.001479     0.001297     0.002803\n",
      "Scale:          56134.6     0.001148     0.001140     0.001155\n",
      "Add:            60686.2     0.001927     0.001582     0.004512\n",
      "Triad:          59847.4     0.001712     0.001604     0.002506\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=32 OMP_PROC_BIND=True make runstream STREAM_N=4000000 -o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** What does `OMP_PROC_BIND=close` mean, and why is it a bad choice, not just for this benchmark, but for any streaming kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "The value OMP_PROC_BIND=close means that the assignment goes successively through the available places. It's bad because when the worker thread is not kept in the same place partition, accessing and updating data becomes more expensive due to the distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** I've modified the benchmark, calling it `stream2.c`.  Here's the difference, it's one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267d266\r\n",
      "< #pragma omp parallel for\r\n"
     ]
    }
   ],
   "source": [
    "!diff stream.c stream2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your options for `runstream` to `runstream2` below.  The reported results should be different: why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icc -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include -qopenmp -o stream2 stream2.c -DSTREAM_ARRAY_SIZE=4000000\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "./stream2\n",
      "-------------------------------------------------------------\n",
      "STREAM version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 4000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 30.5 MiB (= 0.0 GiB).\n",
      "Total memory required = 91.6 MiB (= 0.1 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "-------------------------------------------------------------\n",
      "Number of Threads requested = 32\n",
      "Number of Threads counted = 32\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 1406 microseconds.\n",
      "   (= 1406 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:           30604.9     0.002825     0.002091     0.008343\n",
      "Scale:          27027.3     0.002865     0.002368     0.006358\n",
      "Add:            32574.5     0.003018     0.002947     0.003264\n",
      "Triad:          33310.2     0.003581     0.002882     0.007118\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-13 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=32 OMP_PROC_BIND=True  make runstream2 STREAM_N=4000000 -o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "When line 267 `#pragma omp parallel for` is commented, loop `Get initial value for system clock` is not using multi prosessing and thus the performance drops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Now we're going to run stream benchmarks for the GPU.  As above, modify the array size until you believe you are testing streaming bandwidth from memory and not from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -lineinfo -Xcompiler '-fPIC' -O -o streamcu stream.cu -DSTREAM_ARRAY_SIZE=4000000\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "./streamcu\n",
      "-------------------------------------------------------------\n",
      "CSE6230 CUDA STREAM based on version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 4000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 30.5 MiB (= 0.0 GiB).\n",
      "Total memory required = 91.6 MiB (= 0.1 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "Device Number: 0\n",
      "  Device name: Tesla P100-PCIE-16GB\n",
      "  Memory Clock Rate (KHz): 715000\n",
      "  Memory Bus Width (bits): 4096\n",
      "  Peak Memory Bandwidth (GB/s): 732.160000\n",
      "\n",
      "Ordinal of GPUs requested = 0\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 238 microseconds.\n",
      "   (= 238 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:          512281.4     0.000126     0.000125     0.000127\n",
      "Scale:         508400.5     0.000126     0.000126     0.000127\n",
      "Add:           530504.9     0.000182     0.000181     0.000182\n",
      "Triad:         530504.9     0.000182     0.000181     0.000183\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-12 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!make runstreamcu STREAM_N=4000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (2 pts):** This is final time we're running a stream benchmark, I promise.  This benchmark is also for the GPU, but instead of the arrays originating in the GPUs memory, they start on the CPUs memory, and must be transfered to the GPU and back.  This mimics a common design pattern when people try to modify their code for GPUs: identify the bottleneck kernel, and try to \"offload\" it to the GPU, where it will have a higher throughput (once it get's there).  You don't have to modify this run, I just want you to see what bandwidths it reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -ccbin=icpc -lineinfo -Xcompiler '-fPIC' -O -o streamcu2 stream2.cu -DSTREAM_ARRAY_SIZE=1000000\n",
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "stream2.cu(569): warning: variable \"i\" was set but never used\n",
      "\n",
      "./streamcu2\n",
      "-------------------------------------------------------------\n",
      "CSE6230 CUDA STREAM based on version $Revision: 5.10 $\n",
      "-------------------------------------------------------------\n",
      "This system uses 8 bytes per array element.\n",
      "-------------------------------------------------------------\n",
      "Array size = 1000000 (elements), Offset = 0 (elements)\n",
      "Memory per array = 7.6 MiB (= 0.0 GiB).\n",
      "Total memory required = 22.9 MiB (= 0.0 GiB).\n",
      "Each kernel will be executed 10 times.\n",
      " The *best* time for each kernel (excluding the first iteration)\n",
      " will be used to compute the reported bandwidth.\n",
      "Ordinal of GPUs requested = 0\n",
      "  Device name: Tesla P100-PCIE-16GB\n",
      "  Memory Clock Rate (KHz): 715000\n",
      "  Memory Bus Width (bits): 4096\n",
      "  Peak Memory Bandwidth (GB/s): 732.160000\n",
      "\n",
      "-------------------------------------------------------------\n",
      "1.000000 2.000000 0.000000\n",
      "-------------------------------------------------------------\n",
      "Your clock granularity/precision appears to be 1 microseconds.\n",
      "Each test below will take on the order of 1343 microseconds.\n",
      "   (= 1343 clock ticks)\n",
      "Increase the size of the arrays if this shows that\n",
      "you are not getting at least 20 clock ticks per test.\n",
      "-------------------------------------------------------------\n",
      "WARNING -- The above is only a rough guideline.\n",
      "For best results, please be sure you know the\n",
      "precision of your system timer.\n",
      "-------------------------------------------------------------\n",
      "Function    Best Rate MB/s  Avg time     Min time     Max time\n",
      "Copy:            6125.3     0.002630     0.002612     0.002663\n",
      "Scale:           6113.6     0.002638     0.002617     0.002657\n",
      "Add:             6147.4     0.003943     0.003904     0.004006\n",
      "Triad:           6116.4     0.003954     0.003924     0.003999\n",
      "-------------------------------------------------------------\n",
      "Solution Validates: avg error less than 1.000000e-12 on all three arrays\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!make runstreamcu2 STREAM_N=1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the three peak bandwidths that we have *computed* (not the reported values from question 2) -- CPU, GPU with arrays on the GPU, and GPU with arrays on the CPU -- and with the theoretical peak flop/s for the CPU and GPU, compute *effective system balances* and create a plot with rooflines for all three balances overlayed.\n",
    "\n",
    "- The y axis should be absolute Gflop/s, not relative, so we can compare them, and should be labeled \"Gflop/s\"\n",
    "- Label with roofline goes with which balance: \"CPU\", \"GPU\", \"CPU->GPU->CPU\"\n",
    "- The x axis should be in units of \"double precision flops / byte\"\n",
    "\n",
    "Save your plot as the jpg `threerooflines.jpg` so that it can embed in the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Three rooflines](./threerooflines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (2 pts):** Remember those particles all the way back in question 1?  Your arithmetic intensity estimate could be placed on the roofline plot for the CPUs, and you could make a judgement about whether the kernel is compute bound or memory bound.\n",
    "\n",
    "Now let's put it to the test.  The `make runcloud` target simulates `NPOINT` particles orbiting the sun for `NT` time steps.  Because these particles are independent, you can optionally \"chunk\" multiple time steps for each particle independent of the other particles.  Doing this reduces the number of memory accesses per flop:  each particle stays in register for `NCHUNK` time steps.\n",
    "\n",
    "Do your best to optimize the throughput of the simulation both in the limit of few particles and many time steps, and in the limit of many particles and few time steps.\n",
    "Do that by modifying the commands below.\n",
    "\n",
    "- Make the simulations each run about a second\n",
    "- Do your best to optimize the compiler flags and the runtime (openMP) environment\n",
    "\n",
    "Using the outputs of those runs, estimate the floating point efficiency of our particle-time-step kernel: compare the peak flop/s of the CPU, to the product of particle time steps per second and your estimate of the flops per particle time step. and divide by the throughput of particle time steps per second.  Give that effective arithmetic intensity below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f cloud cloud.o verlet.o\n",
      "make verlet.o DEFINES=\"-DNT=1000\"\n",
      "make[1]: Entering directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "icc -std=c99 -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include -DNT=1000 -qopenmp -c -o verlet.o verlet.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "make[1]: Leaving directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "make cloud\n",
      "make[1]: Entering directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "icc -std=c99 -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include  -qopenmp -c -o cloud.o cloud.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icpc -qopenmp -o cloud verlet.o cloud.o -Wl,-rpath,.\n",
      "make[1]: Leaving directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "./cloud 64 4000000 0.01 1000\n",
      "./cloud, NUM_POINTS=64, NUM_STEPS=4000000, DT=0.01, NCHUNK=1000\n",
      "[./cloud]: 8.766708e-01 elapsed seconds\n",
      "[./cloud]: 2.920138e+08 particle time steps per second\n",
      "[./cloud]: 2.920138e+05 particle time step chunks per second\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=32 OMP_PROC_BIND=True OMP_PLACES=cores make runcloud NPOINT=64 NT=4000000 NCHUNK=1000 -o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f cloud cloud.o verlet.o\n",
      "make verlet.o DEFINES=\"-DNT=2\"\n",
      "make[1]: Entering directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "icc -std=c99 -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include -DNT=2 -qopenmp -c -o verlet.o verlet.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "make[1]: Leaving directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "make cloud\n",
      "make[1]: Entering directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "icc -std=c99 -g -Wall -fPIC -O -qopt-report=3 -I/usr/local/pacerepov1/cuda/8.0.44/include  -qopenmp -c -o cloud.o cloud.c\n",
      "icc: remark #10397: optimization reports are generated in *.optrpt files in the output location\n",
      "icpc -qopenmp -o cloud verlet.o cloud.o -Wl,-rpath,.\n",
      "make[1]: Leaving directory `/nv/coc-ice/zjiang333/cse6230-hw/3-streams-and-roofs'\n",
      "./cloud 6400000 100 0.01 2\n",
      "./cloud, NUM_POINTS=6400000, NUM_STEPS=100, DT=0.01, NCHUNK=2\n",
      "[./cloud]: 7.600188e-01 elapsed seconds\n",
      "[./cloud]: 8.420844e+08 particle time steps per second\n",
      "[./cloud]: 4.210422e+08 particle time step chunks per second\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=32 OMP_PROC_BIND=True OMP_PLACES=cores make runcloud NPOINT=6400000 NT=100 NCHUNK=2 -o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "Theoritical CPU peak flops is:   \n",
    "$8 (processor) * 4 (cores) * 3.20 GHz (frequeny, w/ turbo boost) * 32 (FLOPS/clock) = 3,276.8 GFLOPS$   \n",
    "and effective FLOPS with `NPOINT=6400000 NT=100` is:   \n",
    "$8.4 e^8 \\times 21 = 17.64 GFLOPS$.   \n",
    "The effective arithmetic intensity is:  \n",
    "$21/(72+32+16+24+80) * 2 = 0.1875$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
